{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv, string\n",
    "import imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getData(path):\n",
    "    data = []\n",
    "    with open(path) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            u = row[1]\n",
    "            data.append(u)\n",
    "    data = data[1:]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23121 2893 2862\n"
     ]
    }
   ],
   "source": [
    "train_data_path = './data/propernames/train/train_data.csv'\n",
    "dev_data_path   = './data/propernames/dev/dev_data.csv'\n",
    "test_data_path = './data/propernames/test/test_data.csv'\n",
    "\n",
    "train_data = getData(train_data_path)\n",
    "dev_data = getData(dev_data_path)\n",
    "test_data = getData(test_data_path)\n",
    "\n",
    "print len(train_data), len(dev_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddict = set()\n",
    "lo, hi = 2, 4\n",
    "for row in train_data:\n",
    "    for j in range(lo, hi+1):\n",
    "        for i in range(len(row)-j+1):\n",
    "            ddict.add(row[i:i+j])\n",
    "for row in dev_data:\n",
    "    for j in range(lo, hi+1):\n",
    "        for i in range(len(row)-j+1):\n",
    "            ddict.add(row[i:i+j])\n",
    "dddict = list(ddict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64327"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dddict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def character_N_gram(data, n=2, dict=dddict, sset=ddict):\n",
    "    train = np.zeros((len(data),len(dict)))\n",
    "    for i,row in enumerate(data):\n",
    "        for k in range(2,5):\n",
    "            for j in range(len(row)-k+1):\n",
    "                cur = row[j:j+k]\n",
    "                if cur in sset:\n",
    "                    train[i,dict.index(cur)] = 1\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer='char_wb',ngram_range=(2,4),lowercase=False)\n",
    "count_vect.fit(train_data)\n",
    "train_x = count_vect.transform(train_data)\n",
    "dev_x   = count_vect.transform(dev_data)\n",
    "test_x  = count_vect.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23121, 72507)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x = character_N_gram(train_data)\n",
    "dev_x   = character_N_gram(dev_data)\n",
    "test_x  = character_N_gram(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_label_path = './data/propernames/train/train_labels.csv'\n",
    "dev_label_path   = './data/propernames/dev/dev_labels.csv'\n",
    "\n",
    "def getLabel(path, n):\n",
    "    label = np.zeros(n)\n",
    "    map = [\"person\", \"place\", \"movie\", \"drug\", \"company\"]\n",
    "    with open(path) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        start = 0\n",
    "        for row in reader:\n",
    "            if start == 0: \n",
    "                start += 1\n",
    "                continue\n",
    "            label[start-1] = map.index(row[1])\n",
    "            start += 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y = getLabel(train_label_path, 23121)\n",
    "dev_y   = getLabel(dev_label_path, 2893)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "all_folds = cross_validation.KFold(train_x.shape[0], n_folds = 5)\n",
    "for a,b in all_folds:\n",
    "    perceptron = foo.Perceptron(eta = 0.01, epochs = 5, n_class = 5)\n",
    "    perceptron.train(train_x[a],train_y[a])\n",
    "    pre = perceptron.predict(train_x[b])\n",
    "    cnt = pre.shape[0]\n",
    "    correct = 0\n",
    "    for i in range(cnt):\n",
    "        if pre[i] == train_y[b][i]:\n",
    "            correct += 1\n",
    "    print str(correct)+\"/\"+str(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foo = imp.load_source('Perceptron', '/Users/GanHong/Desktop/CS5740_NLP/Assignment1/models/Perceptron.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components == 'mle')\n",
    "pca.fit(train_x.toarray())\n",
    "train_x = pca.transform(train_x.toarray())\n",
    "test_x  = pca.transform(test_x.toarray())\n",
    "dev_x   = pca.transform(dev_x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.784306947805\n",
      "0.788800553059\n",
      "0.795022468026\n",
      "0.797442101625\n",
      "0.805046664362\n",
      "0.805046664362\n",
      "0.805046664362\n",
      "0.808157621846\n",
      "0.810231593502\n",
      "0.807811959903\n"
     ]
    }
   ],
   "source": [
    "# baseline using perceptron with character 2 gram is 70% accuracy\n",
    "# mix 2-4 character n-garm is 80% accuracy\n",
    "perceptron = foo.Perceptron(n_class = 5, epochs = 1, eta = 0.001, dim = 72507)\n",
    "for _ in range(10):\n",
    "    perceptron.train(train_x.toarray(), train_y.toarray())\n",
    "    res = perceptron.predict(dev_x.toarray())\n",
    "    cnt = res.shape[0]\n",
    "    correct = 0\n",
    "    for i in range(cnt):\n",
    "        if res[i] == dev_y[i]:\n",
    "            correct += 1\n",
    "    print float(correct) / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = perceptron.predict(test_x)\n",
    "ret = []\n",
    "map = [\"person\", \"place\", \"movie\", \"drug\", \"company\"]\n",
    "for i,k in enumerate(out):\n",
    "    ret.append(map[int(k)])\n",
    "np.savetxt('test1.csv', ret, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('test1.csv', range(out.shape[0]), fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxEnt = imp.load_source('MaxEnt', '/Users/GanHong/Desktop/CS5740_NLP/Assignment1/models/MaxEnt.py')\n",
    "me = maxEnt.MaxEnt(n_class = 5)\n",
    "me.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = me.predict(dev_x)\n",
    "cnt = res.shape[0]\n",
    "correct = 0\n",
    "for i in range(cnt):\n",
    "    print res[i], dev_y[i]\n",
    "    if res[i] == dev_y[i]:\n",
    "        correct += 1\n",
    "print str(correct)+\"/\"+str(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_list = []\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "res = None\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 512\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "# 1024 - 512  ------ 75%   100 epochs   batch-size = 1000\n",
    "# 2048 - 1024 ------ 78%   150 epochs   batch-size = 1000\n",
    "n_hidden_1 = 2048\n",
    "n_hidden_2 = 1024\n",
    "n_input = 3025\n",
    "n_classes = 5\n",
    "beta = 0.001\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 3025])\n",
    "y = tf.placeholder(tf.float32, [None, 5])\n",
    "\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)\n",
    "                      +tf.nn.l2_loss(weights['h1'])\n",
    "                      +tf.nn.l2_loss(weights['h2'])\n",
    "                      +tf.nn.l2_loss(biases['b1'])\n",
    "                      +tf.nn.l2_loss(biases['b2']))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "train_f_y = np.zeros((train_x.shape[0],5))\n",
    "for i,j in enumerate(train_y):\n",
    "    train_f_y[i,int(j)] = 1\n",
    "\n",
    "dev_f_y = np.zeros((dev_x.shape[0],5))\n",
    "for i,j in enumerate(dev_y):\n",
    "    dev_f_y[i,int(j)] = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(23121/batch_size)\n",
    "        for _ in range(total_batch):\n",
    "            # Loop over all batches\n",
    "            seq = np.arange(train_x.shape[0])\n",
    "            np.random.shuffle(seq)\n",
    "            seq = seq[:batch_size]\n",
    "        \n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: train_x[seq],\n",
    "                                                          y: train_f_y[seq]})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost)\n",
    "                \n",
    "        # Test model\n",
    "        if epoch % 1 == 0:\n",
    "            correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "            # Calculate accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            print(\"Accuracy:\", accuracy.eval({x: dev_x, y: dev_f_y}))\n",
    "            \n",
    "        res = sess.run(tf.argmax(pred, 1), feed_dict={x: test_x})\n",
    "        \n",
    "    print \"Optimization Finished!\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ret = []\n",
    "map = [\"person\", \"place\", \"movie\", \"drug\", \"company\"]\n",
    "for i,k in enumerate(res):\n",
    "    ret.append(map[int(k)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('test.csv', ret, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inn = np.genfromtxt('testy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map = [\"person\", \"place\", \"movie\", \"drug\", \"company\"]\n",
    "ret = []\n",
    "for i,k in enumerate(inn):\n",
    "    ret.append(map[int(k)])\n",
    "np.savetxt('test1.csv', ret, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from numpy import linalg as LA\n",
    "\n",
    "class Word2Vec:\n",
    "   def loadData(self, data, label):\n",
    "       self.rawData = data\n",
    "       self.rawLabel = label\n",
    "       \n",
    "   def getWordVector(self, s):\n",
    "       r = requests.get('http://10.128.6.73:5000/word2vec/model?word=' + s)\n",
    "       if len(r.text) < 100:\n",
    "           return np.ones((300))\n",
    "       r = base64.decodestring(r.text)\n",
    "       q = np.frombuffer(r, dtype=np.float32)\n",
    "       return q\n",
    "   \n",
    "   def parse(self, s):\n",
    "       tokens = word_tokenize(s.decode('utf-8'))\n",
    "       ret = np.zeros((300))\n",
    "       for w in tokens:\n",
    "           ret = ret + self.getWordVector(w)\n",
    "       \n",
    "       length = LA.norm(ret)\n",
    "       ret = ret/length\n",
    "       return ret\n",
    "       \n",
    "   def generate(self):\n",
    "       nData = len(self.rawData)\n",
    "       self.hashTable = {}\n",
    "       self.featureData = np.zeros((nData, 300))\n",
    "       self.featureLabel = np.zeros((nData))\n",
    "                   \n",
    "       m = len(self.hashTable)\n",
    "       self.m = m\n",
    "       \n",
    "       for i,x in enumerate(self.rawData):\n",
    "           self.parse(x)\n",
    "           \n",
    "           if i%100 == 0:\n",
    "               print(str(i)+'/'+str(nData))\n",
    "       \n",
    "       return self.featureData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx = Word2Vec()\n",
    "xx.loadData(train_data, train_y)\n",
    "train_x_p = xx.generate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
