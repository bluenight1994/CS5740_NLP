{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv, string\n",
    "import imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getData(path):\n",
    "    data = []\n",
    "    with open(path) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            u = row[1]\n",
    "            data.append(u)\n",
    "    data = data[1:]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23121 2893 2862\n"
     ]
    }
   ],
   "source": [
    "train_data_path = './data/propernames/train/train_data.csv'\n",
    "dev_data_path   = './data/propernames/dev/dev_data.csv'\n",
    "test_data_path = './data/propernames/test/test_data.csv'\n",
    "\n",
    "train_data = getData(train_data_path)\n",
    "dev_data = getData(dev_data_path)\n",
    "test_data = getData(test_data_path)\n",
    "\n",
    "print len(train_data), len(dev_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ddict = set()\n",
    "# lo, hi = 2, 4\n",
    "# for row in train_data:\n",
    "#     for j in range(lo, hi+1):\n",
    "#         for i in range(len(row)-j+1):\n",
    "#             ddict.add(row[i:i+j])\n",
    "# for row in dev_data:\n",
    "#     for j in range(lo, hi+1):\n",
    "#         for i in range(len(row)-j+1):\n",
    "#             ddict.add(row[i:i+j])\n",
    "# dddict = list(ddict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def character_N_gram(data, n=2, dict=dddict, sset=ddict):\n",
    "    train = np.zeros((len(data),len(dict)))\n",
    "    for i,row in enumerate(data):\n",
    "        for k in range(2,5):\n",
    "            for j in range(len(row)-k+1):\n",
    "                cur = row[j:j+k]\n",
    "                if cur in sset:\n",
    "                    train[i,dict.index(cur)] = 1\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(analyzer='char', ngram_range=(2,4), lowercase=False, stop_words='english')\n",
    "count_vect.fit(train_data)\n",
    "train_x = count_vect.transform(train_data)\n",
    "dev_x   = count_vect.transform(dev_data)\n",
    "test_x  = count_vect.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count_vect = TfidfVectorizer(analyzer='char', ngram_range=(2,4), lowercase=False, stop_words='english')\n",
    "count_vect.fit(train_data)\n",
    "train_x = count_vect.transform(train_data)\n",
    "dev_x   = count_vect.transform(dev_data)\n",
    "test_x  = count_vect.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23121, 87232)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x = character_N_gram(train_data)\n",
    "dev_x   = character_N_gram(dev_data)\n",
    "test_x  = character_N_gram(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_label_path = './data/propernames/train/train_labels.csv'\n",
    "dev_label_path   = './data/propernames/dev/dev_labels.csv'\n",
    "\n",
    "def getLabel(path, n):\n",
    "    label = np.zeros(n)\n",
    "    map = [\"person\", \"place\", \"movie\", \"drug\", \"company\"]\n",
    "    with open(path) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        start = 0\n",
    "        for row in reader:\n",
    "            if start == 0: \n",
    "                start += 1\n",
    "                continue\n",
    "            label[start-1] = map.index(row[1])\n",
    "            start += 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y = getLabel(train_label_path, 23121)\n",
    "dev_y   = getLabel(dev_label_path, 2893)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "all_folds = cross_validation.KFold(train_x.shape[0], n_folds = 5)\n",
    "for a,b in all_folds:\n",
    "    perceptron = foo.Perceptron(eta = 0.01, epochs = 5, n_class = 5)\n",
    "    perceptron.train(train_x[a],train_y[a])\n",
    "    pre = perceptron.predict(train_x[b])\n",
    "    cnt = pre.shape[0]\n",
    "    correct = 0\n",
    "    for i in range(cnt):\n",
    "        if pre[i] == train_y[b][i]:\n",
    "            correct += 1\n",
    "    print str(correct)+\"/\"+str(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foo = imp.load_source('Perceptron', '/Users/GanHong/Desktop/CS5740_NLP/Assignment1/models/Perceptron.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components = 'mle')\n",
    "# pca.fit(train_x.toarray())\n",
    "# train_x = pca.transform(train_x.toarray())\n",
    "# test_x  = pca.transform(test_x.toarray())\n",
    "# dev_x   = pca.transform(dev_x.toarray())\n",
    "train_x = train_x.toarray()\n",
    "dev_x   = dev_x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.63102806972\n",
      "0.738333909437\n",
      "loss: 0.810259071839\n",
      "0.815416522641\n",
      "loss: 0.863154707841\n",
      "0.819564465952\n",
      "loss: 0.893516716405\n",
      "0.825095057034\n",
      "loss: 0.91116301198\n",
      "0.825440718977\n",
      "loss: 0.92612776264\n",
      "0.830625648116\n",
      "loss: 0.935210414774\n",
      "0.842723816108\n",
      "loss: 0.943125297349\n",
      "0.84168683028\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1d4331b63e6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mperceptron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m87232\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mperceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GanHong/Desktop/CS5740_NLP/Assignment1/models/Perceptron.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5001\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5002\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5003\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# baseline using perceptron with character 2 gram is 70% accuracy\n",
    "# mix 2-4 character n-garm is 80% accuracy\n",
    "\n",
    "perceptron = foo.Perceptron(n_class = 5, epochs = 1, eta = 0.0001, dim = 87232)\n",
    "for _ in range(20):\n",
    "    perceptron.train(train_x, train_y)\n",
    "    res = perceptron.predict(dev_x)\n",
    "    cnt = res.shape[0]\n",
    "    correct = 0\n",
    "    for i in range(cnt):\n",
    "        if res[i] == dev_y[i]:\n",
    "            correct += 1\n",
    "    print float(correct) / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_x = test_x.toarray()\n",
    "out = perceptron.predict(test_x)\n",
    "ret = []\n",
    "map = [\"person\", \"place\", \"movie\", \"drug\", \"company\"]\n",
    "for i,k in enumerate(out):\n",
    "    ret.append(map[int(k)])\n",
    "np.savetxt('test1.csv', ret, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('test1.csv', range(out.shape[0]), fmt=\"%i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxEnt = imp.load_source('MaxEnt', '/Users/GanHong/Desktop/CS5740_NLP/Assignment1/models/MaxEnt.py')\n",
    "me = maxEnt.MaxEnt(n_class = 5)\n",
    "me.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = me.predict(dev_x)\n",
    "cnt = res.shape[0]\n",
    "correct = 0\n",
    "for i in range(cnt):\n",
    "    print res[i], dev_y[i]\n",
    "    if res[i] == dev_y[i]:\n",
    "        correct += 1\n",
    "print str(correct)+\"/\"+str(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "res = None\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 512\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "# 1024 - 512  ------ 75%   100 epochs   batch-size = 1000\n",
    "# 2048 - 1024 ------ 78%   150 epochs   batch-size = 1000\n",
    "n_hidden_1 = 2048\n",
    "n_hidden_2 = 1024\n",
    "n_input = 87232\n",
    "n_classes = 5\n",
    "beta = 0.001\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, 3025])\n",
    "y = tf.placeholder(tf.float32, [None, 5])\n",
    "\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)\n",
    "                      +beta*tf.nn.l2_loss(weights['h1'])\n",
    "                      +beta*tf.nn.l2_loss(weights['h2'])\n",
    "                      +beta*tf.nn.l2_loss(biases['b1'])\n",
    "                      +beta*tf.nn.l2_loss(biases['b2']))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "train_f_y = np.zeros((train_x.shape[0],5))\n",
    "for i,j in enumerate(train_y):\n",
    "    train_f_y[i,int(j)] = 1\n",
    "\n",
    "dev_f_y = np.zeros((dev_x.shape[0],5))\n",
    "for i,j in enumerate(dev_y):\n",
    "    dev_f_y[i,int(j)] = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(23121/batch_size)\n",
    "        for _ in range(total_batch):\n",
    "            # Loop over all batches\n",
    "            seq = np.arange(train_x.shape[0])\n",
    "            np.random.shuffle(seq)\n",
    "            seq = seq[:batch_size]\n",
    "        \n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: train_x[seq],\n",
    "                                                          y: train_f_y[seq]})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost)\n",
    "                \n",
    "        # Test model\n",
    "        if epoch % 1 == 0:\n",
    "            correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "            # Calculate accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            print(\"Accuracy:\", accuracy.eval({x: dev_x, y: dev_f_y}))\n",
    "            \n",
    "        res = sess.run(tf.argmax(pred, 1), feed_dict={x: test_x})\n",
    "        \n",
    "    print \"Optimization Finished!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
